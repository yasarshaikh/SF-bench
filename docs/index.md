---
layout: default
title: SF-Bench - Salesforce AI Benchmark
description: The open benchmark for evaluating AI coding agents on Salesforce development. Objective measurement, real execution, verified results.
---

# SF-Bench: The Salesforce AI Benchmark

**The open, objective benchmark for measuring AI coding agents on Salesforce development tasks.**

---

## üìå Quick Navigation

| I want to... | Link |
|--------------|------|
| üèÜ See results | [Leaderboard](#-leaderboard) |
| üß™ Test my model | [Testing Your Model](#-testing-your-model) |
| ‚ûï Add tasks | [Contributing](https://github.com/yasarshaikh/SF-bench/blob/main/CONTRIBUTING.md) |
| üìä Submit results | [Submit Results](https://github.com/yasarshaikh/SF-bench/issues/new?template=submit-results.md) |

---

## üéØ What We Do

SF-Bench **measures and reports**. We don't predict or claim expected outcomes.

| We Do | We Don't |
|-------|----------|
| ‚úÖ Measure actual performance | ‚ùå Predict success rates |
| ‚úÖ Report objective results | ‚ùå Claim what models "should" score |
| ‚úÖ Verify functional outcomes | ‚ùå Interpret or editorialize |
| ‚úÖ Test in real Salesforce orgs | ‚ùå Just check syntax |

---

## üèÜ Leaderboard

*December 2025*

| Rank | Model | Overall | Functional Score | LWC | Deploy | Apex | Flow | Lightning Pages | Experience Cloud | Architecture |
|:----:|-------|:-------:|:----------------:|:---:|:------:|:----:|:----:|:---------------:|:----------------:|:------------:|
| ü•á | **Claude Sonnet 4.5** | **41.67%** | **6.0%** | 100% | 100% | 100% | 0%* | 0% | 0% | 0% |
| ü•à | **Gemini 2.5 Flash** | **25.0%** | - | 100% | 100% | 0%* | 0%* | 0% | 0% | 0% |
| - | *More results pending* | -% | - | -% | -% | -% | -% | -% | -% | -% |

*\* Flow tasks failed due to scratch org creation issues (being fixed)*

> **Note**: Functional Score (0-100) uses weighted validation. See [VALIDATION_METHODOLOGY.md](VALIDATION_METHODOLOGY.md) for details.

**[Full Leaderboard ‚Üí](LEADERBOARD.md)**

---

## üß™ Testing Your Model

### Supported Providers

| Provider | Models | Setup |
|----------|--------|-------|
| **OpenRouter** | 100+ models | `OPENROUTER_API_KEY` |
| **RouteLLM** | Gemini 3, Grok, GPT-5 | `ROUTELLM_API_KEY` |
| OpenAI | GPT-4, GPT-3.5 | `OPENAI_API_KEY` |
| Anthropic | Claude 3.5, 3 | `ANTHROPIC_API_KEY` |
| Google | Gemini 2.5, Pro | `GOOGLE_API_KEY` |
| Ollama | Local models | No key needed |

### Quick Start

```bash
git clone https://github.com/yasarshaikh/SF-bench.git
cd SF-bench
pip install -e .

# Run with OpenRouter (access to all models)
export OPENROUTER_API_KEY="your-key"
python scripts/evaluate.py --model anthropic/claude-3.5-sonnet

# Run with Gemini
export GOOGLE_API_KEY="your-key"
python scripts/evaluate.py --model gemini-2.5-flash

# Run with local Ollama
python scripts/evaluate.py --model codellama --provider ollama
```

---

## üîß How Validation Works

### We Check Outcomes, Not Just Deployment

```
Standard Benchmark:
  Deploy succeeded? ‚Üí PASS ‚úÖ
  
SF-Bench:
  Deploy succeeded? ‚Üí Step 1 of 3
  Tests passed? ‚Üí Step 2 of 3
  Business outcome achieved? ‚Üí Step 3 of 3 ‚Üí PASS/FAIL
```

### Example: Flow Task

```bash
# Step 1: Deploy
sf project deploy start  # ‚úÖ

# Step 2: Create test data
sf apex run -c "insert new Account(Name='Test', Type='Customer');"

# Step 3: Verify outcome
sf data query -q "SELECT Id FROM Task WHERE WhatId = :accId"
# 1 Task created ‚Üí PASS
# 0 Tasks ‚Üí FAIL (Flow didn't work)
```

---

## üìä Task Categories

| Category | Tasks | Description |
|----------|:-----:|-------------|
| Apex | 2 | Triggers, Classes |
| LWC | 2 | Lightning Components |
| Flow | 2 | Record-Triggered Flows |
| Lightning Pages | 1 | Dynamic Forms |
| Experience Cloud | 1 | Guest Access |
| Architecture | 4 | Full-stack Design |

---

## üìñ Documentation

- [Validation Methodology](REALISTIC_VALIDATION.md)
- [Benchmark Details](BENCHMARK.md)
- [Leaderboard](LEADERBOARD.md)
- [Contributing](https://github.com/yasarshaikh/SF-bench/blob/main/CONTRIBUTING.md)

---

## ü§ù Get Involved

| Action | Link |
|--------|------|
| ‚≠ê Star the repo | [GitHub](https://github.com/yasarshaikh/SF-bench) |
| üìä Submit results | [Submit](https://github.com/yasarshaikh/SF-bench/issues/new?template=submit-results.md) |
| üêõ Report bugs | [Issues](https://github.com/yasarshaikh/SF-bench/issues) |
| ‚ûï Add tasks | [Contributing](https://github.com/yasarshaikh/SF-bench/blob/main/CONTRIBUTING.md) |

---

**‚≠ê Star us on GitHub if you find SF-Bench useful!**
