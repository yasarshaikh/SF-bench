---
layout: default
title: SF-Bench
description: The first comprehensive benchmark for evaluating AI coding agents on Salesforce development tasks
---

## ğŸ¯ The Problem

**The Salesforce ecosystem represents $50B+ in annual revenue** with millions of developers worldwide. As AI coding assistants become mainstream, there's **no standardized benchmark** to evaluate their effectiveness on Salesforce-specific tasks.

| What Generic Benchmarks Miss | Why It Matters |
|------------------------------|----------------|
| âŒ No Apex/LWC testing | Salesforce's primary languages |
| âŒ No scratch org execution | Real platform validation |
| âŒ No governor limits | Critical platform constraints |
| âŒ No declarative tools | Flows, validation rules, formulas |
| âŒ No enterprise patterns | Triggers, batch jobs, integrations |

---

## âœ… The Solution

**SF-Bench** fills this gap with:

| Feature | Description |
|---------|-------------|
| ğŸ”„ **Real Execution** | Tests run in actual Salesforce scratch orgs |
| ğŸ“Š **15+ Task Types** | Apex, LWC, Flows, Lightning Pages, and more |
| ğŸ—ï¸ **Architecture-Level** | Evaluates planning and system design |
| âœ”ï¸ **Outcome Validation** | Goes beyond just test passing |
| ğŸ“ˆ **Public Leaderboard** | Compare AI models head-to-head |

---

## ğŸ† Leaderboard

| Rank | Model | Pass Rate | Tasks Passed | Date |
|:----:|-------|:---------:|:------------:|:----:|
| ğŸ¥‡ | *Submit your results* | -% | -/- | - |
| ğŸ¥ˆ | - | - | - | - |
| ğŸ¥‰ | - | - | - | - |

[**ğŸ“Š Submit Your Results â†’**](https://github.com/yasarshaikh/SF-bench/issues/new?template=submit-results.md)

---

## ğŸš€ Quick Start

```bash
# Clone and install
git clone https://github.com/yasarshaikh/SF-bench.git
cd SF-bench
pip install -e .

# Run evaluation
python scripts/evaluate.py --model <your-model> --solutions solutions/<your-model>/
```

**Prerequisites:** Python 3.10+ â€¢ [Salesforce CLI](https://developer.salesforce.com/tools/salesforcecli) â€¢ Node.js 18+ â€¢ Authenticated Dev Hub

---

## ğŸ“Š Task Coverage

| Category | Task Types | Validation |
|----------|------------|------------|
| **Development** | Apex, LWC, Triggers, Batch Jobs | Unit tests, Jest |
| **Declarative** | Flows, Validation Rules, Formulas | Flow validation |
| **Configuration** | Page Layouts, Lightning Pages, Communities | Deploy check |
| **Architecture** | Data Model, Security, Integration | Multi-layer validation |

---

## ğŸ¤ Get Involved

| Action | Link |
|--------|------|
| â­ Star the repo | [GitHub Repository](https://github.com/yasarshaikh/SF-bench) |
| ğŸ“Š Submit results | [Submit Results](https://github.com/yasarshaikh/SF-bench/issues/new?template=submit-results.md) |
| ğŸ› Report bugs | [Issue Tracker](https://github.com/yasarshaikh/SF-bench/issues) |
| ğŸ’¬ Discussions | [GitHub Discussions](https://github.com/yasarshaikh/SF-bench/discussions) |
| ğŸ“ Contribute | [Contributing Guide](https://github.com/yasarshaikh/SF-bench/blob/main/CONTRIBUTING.md) |

---

## ğŸ“– Citation

```bibtex
@software{sfbench2024,
  author = {Shaikh, Yasar},
  title = {SF-Bench: Benchmark for Evaluating AI Coding Agents on Salesforce Development},
  year = {2024},
  url = {https://github.com/yasarshaikh/SF-bench}
}
```

---

**â­ Star us on GitHub if you find SF-Bench useful!**

Made with â¤ï¸ for the Salesforce & AI community
